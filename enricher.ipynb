{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "As of [July 16, 2024](https://its.umich.edu/computing/ai/release-notes) Maizey now supports mixed data sources, meaning this script has been deprecated. It still may be useful in cases where Maizey's default parser produces inaccurate information, as you can manually modify this parser's outputs; that being said, for the majority of pages this will not be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info\n",
    "Custom web parser with proposition extraction capability.\n",
    "This parser is meant for a first pass; for best results, manually audit the results and delete information the parser may have accidentally pulled (i.e. navigation items). Similarly, it may not pull everything, in which case you may have to add content.\n",
    "\n",
    "The pipeline for parsing websites is as follows:\n",
    "1. Download websites into ./unparsed\n",
    "2. Run the parser\n",
    "3. Run pandoc (optional)\n",
    "4. Upload to Pascal's data sources\n",
    "\n",
    "You can download websites using either the scraper or cmd + s (MacOS) or ctrl + s (Windows, Linux) in most web browsers. Make sure to delete non-HTML files from ./unparsed before running.\n",
    "\n",
    "# Quickstart\n",
    "To run this parser, you must have [Jupyter](https://jupyter.org/) installed. We recommend installing Jupyter Notebook. Then, create a directory with this parser and two empty folders titled 'parsed' and 'unparsed'. Download any websites you want parsed into 'unparsed'. Then, run the parser via Jupyter Notebook.\n",
    "\n",
    "# Troubleshooting\n",
    "If you get an error while running this script, it is likely that there was an unresolved case (for example, the indicator for table rows was not included in the parser). You can either remove the offending website or modify the parser directly to resolve this.\n",
    "\n",
    "# Using an LLM\n",
    "This parser allows you to optionally use an LLM to perform proposition extraction.\n",
    "The idea is that we can pull the most relevant information from a website using an LLM.\n",
    "This produces superior results when these documents are used by Maizey.\n",
    "However, it requires a GPT Toolkit API key and is not strictly necessary.\n",
    "We opted for this technique in the inaugural batch of documents as we already had a token and wanted to use it as a proof-of-concept. It is not recommended for every use case.\n",
    "\n",
    "## Pros\n",
    "- Measurably superior retrieval\n",
    "\n",
    "## Cons\n",
    "- Requires an API key\n",
    "- More expensive\n",
    "- Harder to work with\n",
    "\n",
    "If you decide you want to use an LLM, this script requires a .env with the following info:\n",
    "\n",
    "model=**any model**\n",
    "\n",
    "embedder=**text-embedding-3-small**\n",
    "\n",
    "azure_endpoint=https://api.umgpt.umich.edu/azure-openai-api\n",
    "\n",
    "OPENAI_API_KEY=[api key]\n",
    "\n",
    "OPENAI_organization=**shortcode**\n",
    "\n",
    "API_VERSION=2023-05-15\n",
    "\n",
    "Replace the bolded text with the corresponding information. Then, switch use_llm to **True** and modify the prompt as you see fit.\n",
    "\n",
    "# Convert to .docx\n",
    "\n",
    "The parser returns .md files by default. While Maizey is able to use .md files, you cannot edit them in Google Docs and they are not formatted in Google Drive's viewer. We therefore recommend using a tool like [pandoc](https://pandoc.org/) to convert these .md files into .docx, a format which Google Docs supports. You can refer to [pandoc's official documentation](https://pandoc.org/installing.html) for information about how to install pandoc. We've also provided a script to run it on all .md files in your current directory below.\n",
    "\n",
    "`for f in *.md; do pandoc \"$f\" -s -o \"${f%.md}.docx\"; done`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "### SETTINGS ###\n",
    "use_llm = True\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant. When given a document, extract every proposition into a list and summarize the document afterwards. Keep your answers as short as possible without losing any meaning.\n",
    "If you are given links, make sure to include them in your list, but not the summary.\n",
    "Preface your response with \"Propositions:\" or \"Summary:\".\n",
    "Do not include any other information in your response.\n",
    "Do not comment on the quality of the document.\n",
    "Do not include any information that is not a proposition or a summary.\n",
    "Put \\\\n\\\\n between each proposition.\n",
    "\"\"\"\n",
    "\n",
    "if use_llm:\n",
    "    notebook_dir = os.path.dirname(os.path.abspath('parse.ipynb'))\n",
    "    os.chdir(notebook_dir)\n",
    "    \n",
    "    try:\n",
    "        if load_dotenv('.env') is False:\n",
    "            raise TypeError\n",
    "    except TypeError:\n",
    "        print('Unable to load .env file.')\n",
    "        quit()\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key=os.environ['OPENAI_API_KEY'],  \n",
    "        api_version=os.environ['API_VERSION'],\n",
    "        azure_endpoint = os.environ['azure_endpoint'],\n",
    "        organization = os.environ['OPENAI_organization']\n",
    "    )\n",
    "\n",
    "    def query(sysprompt = '', doc = ''):\n",
    "        response = client.chat.completions.create(\n",
    "            model=os.environ['model'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sysprompt},\n",
    "                {\"role\": \"user\", \"content\": doc}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            stop=None)\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def extractPropositions(doc):\n",
    "        return query(prompt, ''.join(doc[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './unparsed'\n",
    "\n",
    "def convert_to_markdown(element):\n",
    "    links = \"\"\n",
    "    if element.name == 'a':\n",
    "        url = element.get('href')\n",
    "        text = element.text\n",
    "        if url:\n",
    "            links += f\"{text}({url})\"\n",
    "    else:\n",
    "        for child in element.descendants:\n",
    "            if child.name == 'a':\n",
    "                url = child.get('href')\n",
    "                text = child.text\n",
    "                if url:\n",
    "                    links += f\"({url})\"\n",
    "    return element.text + links\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.html'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        with codecs.open(filepath, 'r', 'utf-8') as file:\n",
    "            content = file.read()\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "        print(soup.title.string if soup.title else 'No title found')\n",
    "\n",
    "        markdown_lines = []\n",
    "        canonical_link = soup.find(\"link\", {\"rel\": \"canonical\"})\n",
    "        if canonical_link:\n",
    "            link_text = f\"Link to original: [{canonical_link['href']}]({canonical_link['href']})\\n\\n\"\n",
    "            markdown_lines.append(link_text)\n",
    "        else:\n",
    "            og_link = soup.find(\"meta\", {\"property\": \"og:url\"})\n",
    "            if og_link:\n",
    "                link_text = f\"Link to original: [{og_link['content']}]({og_link['content']})\\n\\n\"\n",
    "                markdown_lines.append(link_text)\n",
    "\n",
    "        body = soup.find(\"body\")\n",
    "\n",
    "        for nav in body.find_all('nav'):\n",
    "            nav.decompose()\n",
    "\n",
    "        elements = body.find_all(['table', 'p', 'h1', 'li'])\n",
    "        for element in elements:\n",
    "            if element.name == 'table':\n",
    "                rows = element.find('tbody').findAll('tr', recursive=False)\n",
    "                for row in rows:\n",
    "                    cells = row.findAll('td')\n",
    "                    for cell in cells:\n",
    "                        nl = '\\n'\n",
    "                        try:\n",
    "                            cell_text = f'{cell[\"data-th\"]}: {\"; \".join(cell.text.strip().split(nl))}\\n\\n'\n",
    "                        except:\n",
    "                            cell_text = f'{\"; \".join(cell.text.strip().split(nl))}\\n\\n'\n",
    "                        markdown_lines.append(cell_text)\n",
    "            elif element.name in ['p', 'li']:\n",
    "                if element.text.strip() == \"\":\n",
    "                    continue\n",
    "                parentnames = [parent.name for parent in element.parents]\n",
    "                if 'li' not in parentnames:\n",
    "                    formatted_line = convert_to_markdown(element).strip()\n",
    "                    markdown_lines.append(\"- \" + formatted_line + \"\\n\\n\")\n",
    "            elif element.name == 'h1':\n",
    "                header_text = f\"# {element.text}\\n\\n\"\n",
    "                markdown_lines.append(header_text)\n",
    "        \n",
    "        if use_llm:\n",
    "            propositions = extractPropositions(markdown_lines)\n",
    "            markdown_lines.append('\\n\\n')\n",
    "            markdown_lines.append(propositions)\n",
    "\n",
    "        output_path = os.path.join('./parsed/', filename[:-5] + '.md')\n",
    "        with open(output_path, 'w', encoding='utf-8') as md_file:\n",
    "            md_file.write(''.join(markdown_lines))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
